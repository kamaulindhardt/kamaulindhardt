---
title: "Playing around with the {SITS} package"
description: |
  In this blog post I am playing around with the package sits (Satellite Image Time Series Analysis for Earth Observation Data Cubes). 
author:
  - name: Kamau Lindhardt
date: 11-21-2021
preview: sits_general_view.png
draft: false #false
output:
  distill::distill_article:
    self_contained: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introduction

Earth observation (EO) satellites provide a common and consistent set of information about the planet’s land and oceans. Recently, most space agencies have adopted open data policies, making unprecedented amounts of satellite data available for research and operational use. This data deluge has brought about a significant challenge: How to design and build technologies that allow the Earth observation community to analyse big data sets?

In this blog post I am playing around with the package SITS (Satellite Image Time Series Analysis for Earth Observation Data Cubes). SITS, an open-source R package for satellite image time series analysis. It provides support on how to use machine learning techniques with image time series by using a time-first, space-later approach, where each spatial location is associated to a time series. A set of locations with known labels is used to train a machine learning classifiers. The resulting model is applied to the data cube and each time series is classified separately. After the classification, spatial smoothing methods capture information from neighbours.The package supports the complete cycle of data analysis for time series classification, including data acquisition, visualization, filtering, clustering, classification, validation, and post-processing.The sits R package provides a set of tools for analysis, visualization and classification of satellite image time series. The package supports classification of image data cubes using machine learning methods. The basic workflow in SITS is:

1. Create a data cube using image collections available in the cloud or in local machines.
2. Extract time series from the data cube which are used as training data.
3. Perform quality control and filtering on the samples.
4. Train a machine learning model using the extracted samples.
5. Classify the data cube using the trained model.
6. Post-process the classified images.
7. Evaluate the accuracy of the classification using best practices.

```{r, out.width="50%", fig.cap="Using time series for land classification (source: authors of SITS)"}
knitr::include_graphics(here::here("_posts/2021-11-21-sits/sits_general_view.png"))
```

# Installing and loading the package and data

**The SITS package can be downloaded and installed from GitHub:**

```{r, eval=FALSE, echo=TRUE}
devtools::install_github("e-sensing/sits", dependencies = TRUE)
```

**The “sitsdata” package, which contains all exemplary data:**

```{r, eval=FALSE, echo=TRUE}
devtools::install_github("e-sensing/sitsdata")
```

**Loading the sits package and data**

```{r, eval=TRUE, echo=TRUE}
library(sits)
library(sitsdata)
```

**Installing/loading other packages**

```{r}
library(keras)
```


# 0.1 Creating a Data Cube

**Creating a raster metadata file based on the information about the files**

When building data cubes from images stored in a local machine, users need to provide information about the original source from with the data was downloaded. The reason to included such information is because there are no standards for the metadata used to process an image. Unfortunately, information such as band names, maximum/minimum values, and cloud pixels are set by the provider. When sits accesses local data, it needs to know where the data comes from. Hence the need to include origin and collection parameters when defining a local cube.

```{r, eval=TRUE, echo=TRUE}
data_dir <- system.file("extdata/sinop", package = "sitsdata")

# create a raster metadata file based on the information about the files
sinop_cube <- sits_cube(source = "LOCAL",
                   origin = "BDC",
                   collection  = "MOD13Q1-6",
                   name = "Sinop",
                   data_dir = data_dir,
                   parse_info = c("X1", "X2", "tile", "band", "date")
)
```

**Plotting an EVI band**

Plotting the EVI band for the first date (2013-09-14)

```{r}
# plot the EVI band for the first date (2013-09-14)
plot(sinop_cube, band = "EVI", time = 1)
```

The sits_cube() function defines a data cube, which is an organized collection of images covering a geographical area in a given time interval. Data cubes can be conceived as a 3D array of pixels, where each pixel is associated to a time series. All pixels share the same timeline and the same set of attributes (usually spectral bands). When a data cube is defined, the values of the images are not loaded in memory. The output of sits_cube() is a table which contains the metadata that describes the actual image data.


# 0.2 The time series table

To classify all of the time series associated to a data cube, sits uses machine learning models. To train the models, sits uses a tabular data structure that stores individual time series. The example below shows a table with 1,218 time series obtained from MODIS MOD13Q1 images. Each series has four attributes: two bands (“NIR” and “MIR”) and two indexes (“NDVI” and “EVI”).

**load the MODIS samples for Mato Grosso from the "sitsdata" package**

```{r}
# load the MODIS samples for Mato Grosso from the "sitsdata" package
# library(sitsdata)
data("samples_matogrosso_mod13q1", package = "sitsdata")
samples_matogrosso_mod13q1[1:10,]
```

The data structure associated to the time series is a table that contains data and metadata. The first six columns contain the metadata: spatial and temporal information, the label assigned to the sample, and the data cube from where the data has been extracted. The time_series column contains the time series data for each spatiotemporal location. This data is also organized as a table, with a column with the dates and the other columns with the values for each spectral band.

It is useful to visualize the dispersion of the time series. In what follows, for brevity we will select only one label (“Forest”) and one index (“EVI”) to show. The resulting plot shows all of the time series associated to the label and attribute, highlighting the median and the first and third quartiles.

```{r}
samples_forest <- dplyr::filter(samples_matogrosso_mod13q1, label == "Forest")
samples_pasture <- dplyr::filter(samples_matogrosso_mod13q1, label == "Pasture")
samples_soy_corn <- dplyr::filter(samples_matogrosso_mod13q1, label == "Soy_Corn")

samples_forest_ndvi <- sits_select(samples_forest, band = "NDVI")
samples_pasture_ndvi <- sits_select(samples_pasture, band = "NDVI")
samples_soy_corn_ndvi <- sits_select(samples_soy_corn, band = "NDVI")

```


```{r}
plot(samples_forest_ndvi)
```
```{r}
plot(samples_pasture_ndvi)
```
```{r}
plot(samples_soy_corn_ndvi)
```


# 0.3 Training a machine learning model

After obtaining the time series, the next step is to select a suitable subset to use as training samples for a machine learning model. In this case, the time series data has four attributes (“EVI,” “NDVI,” “NIR,” “MIR”) and the data cube is composed only with data from the “NDVI” and “EVI” indexes. We extract the “NDVI” and “EVI” indexes from the time series data set and use the resulting data for training a model. To build the classification model, we have chosen sits_TempCNN() from the methods available. This method implements a 1D convolution neural network^[C. Pelletier, G. I. Webb, and F. Petitjean, “Temporal Convolutional Neural Network for the Classification of Satellite Image Time Series,” Remote Sensing, vol. 11, no. 5, 2019.]. After training the model, we plot the result to how well it has converged to match the input data.

**select the bands "ndvi", "evi" train machine learning classification model**
```{r}
# select the bands "ndvi", "evi"
samples_2bands <- sits_select(samples_matogrosso_mod13q1, bands = c("NDVI", "EVI"))

#select a tempCNN model
tcnn_model <- sits_train(data = samples_2bands, 
                        ml_method = sits_TempCNN())
```
```{r}
plot(tcnn_model)
```


# 0.4 Data cube classification

The next step is to classify the data cube. This is achieved by using the sits_classify() function. The classification produces a set of probability maps, one for each class. For each map, the value of a pixel is proportional to the the probability that it belongs to the class. To visualize the result, we plot the probability maps. In the example below, we show the maps of two classes (“Forest” and “Pasture”).

```{r}
# classify the raster image
sinop_probs <- sits_classify(data = sinop_cube, 
                             ml_model = tcnn_model)

plot(sinop_probs, labels = c("Forest", "Pasture"))
```

# 0.5 Spatial smoothing

When working with big EO data sets, there is a considerable degree of data variability in each class. As a result, some pixels will be misclassified. These errors are more likely to occur in transition areas between classes or when dealing with mixed pixels. To offset these problems, sits includes a post-processing smoothing method based on Bayesian probability. The sits_smooth() function uses information from a pixel’s neighborhood to reduce uncertainty about its label, which is illustrated below. After smoothing, we plot the probability maps for classes “Forest” and “Pasture” to compare with the previous plot. 

perform spatial smoothing

```{r}
# perform spatial smoothing
sinop_smooth <- sits_smooth(sinop_probs)
plot(sinop_smooth, labels = c("Forest", "Pasture"))
```

# 0.6 Labelling a probability data cube

After removing outliers using local smoothing, one can obtain the labeled classification map using the function sits_label_classification(). This function assigns each pixel to the class with highest probability.

```{r}
# label the probability file 
# (by default selecting the class with higher probability)
sinop_label <- sits_label_classification(sinop_smooth)
plot(sinop_label, title = "Sinop-label")
```
The resulting classification files can be read by QGIS. Links to the associated files are available in the sinop_label table in the column file_info.

```{r}
# show the location of the classification file
sinop_label$file_info[[1]]
```

# 0.7 How the sits API works

The core functions of the sits API are presented in Figure 0.7. Each function carries out one task of the land classification workflow. These functions are: (a) sits_cube() which creates a cube; (b) sits_get_data() which extracts training data from the cube; (c) sits_train() that trains a machine learning model; (d) sits_classify() which classifies the cube; (e) sits_smooth() that does the spatial smoothing; and (f) sits_label_classication() that produces the final labelled image. These six functions encapsulate the core of the package. 

```{r, out.width="50%", fig.cap="SITS API (source: authors of SITS)"}
knitr::include_graphics(here::here("_posts/2021-11-21-sits/sits_api.png"))
```


# 0.8 Final remarks

The sits package provides an API to build EO data cubes from image collections available in cloud services, and to perform land classification of data cubes using machine learning. The classification models are built based on satellite image time series extracted from the cubes. The package provides additional function for sample quality control, post-processing and validation. The design of the API tries to reduce complexity for users and hide details such as how to do parallel processing, and to handle data cubes composed by tiles of different timelines.

# Earth observation data cubes


## 1.1  Image data cubes as the basis for big Earth observation data analysis


## 1.2  Analysis-ready data image collections


## 1.3 Accessing Data Cubes and Image Collections in SITS

### 1.3.1 Accessing data cubes in Amazon Web Services

Users of Amazon Web Services (AWS) can access image collections available in the ‘Earth on AWS’ services using sits. For AWS, sits currently works with collection “sentinel-s2-l2a.” This will be extended in later versions.

To work with AWS, users need to provide credentials using environment variables.

```{r}
Sys.setenv(
    "AWS_ACCESS_KEY_ID"     = "AKIAZ2UOEVRZDMF377NJ",
    "AWS_SECRET_ACCESS_KEY" = "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",
    "AWS_DEFAULT_REGION"    = "eu-central-1",
    "AWS_ENDPOINT"          = "dynamodb.eu-central-1",
    "AWS_REQUEST_PAYER"     = "requester"
)
```

Sentinel-2 level 2A files in AWS collection “sentinel-s2-l2a” are organized by sensor resolution. Bands “B02,” “B03,” “B04,” and “B08” AWS are available in 10m resolution. Bands “B02,” “B03,” “B04,” “B05,” “B06,” “BO7,” B08“,”B8A“,”B11“, and”B12" are available at 20m resolution. All 12 bands are available at 60m resolution. Because of the availability of some bands at different resolutions, users need to specify the s2_resolution parameter to create Sentinel-2 images data cubes in AWS using collection “sentinel-s2-l2a.” In the example below, the user selects one Sentinel-2 tile. Each S2 tile is an 100x100 km2 orthoimage in UTM/WGS84 projection.

```{r}
# creating a data cube in AWS
s2_cube <- sits_cube(source = "AWS",
                     name = "T20LKP_2018_2019",
                     collection = "sentinel-s2-l2a",
                     tiles = c("20LKP"),
                     start_date = as.Date("2018-07-18"),
                     end_date = as.Date("2018-07-23"),
                     bands = c("B02", "B03", "B04", "B08", "B11"),
                     s2_resolution = 20
)
```

The output of the sits_cube() function is composed of metadata about the images that satisfy the requirements stated in its parameters (spatiotemporal extent, resolution, and area of interest). The s2_cube object created in the above statement is a tibble that has the information required for further processing, but does not contain the actual data.

Instead of specifying the region of interest by listing the image collection tiles, users can also provide a bounding box (bbox) whose parameters allow a selection of an area of interest. Bounding boxes can be defined using: (a) a named vector (“xmin,” “ymin,” “xmax,” “ymax”) with lat/long values in WGS 84; (b) an sf object from the sf package, a data frame with feature attributes and feature geometries; or (c) a GeoJSON geometry (RFC 7946). When selecting images that compose a data cube based on a bbox, sits does not crop them directly; the software selects the images that intersect with it. The information is used later by sits_classify(), when only the pixels inside the bounding box will be processed.

In SITS version 0.14.0, users who do not want to use the sits_regularize() function and prefer to use AWS images directly can only create valid data cubes if the images belong to the same tile. This limitation will be removed in future versions of the package.

### 1.3.2 Accessing the Brazil Data Cube

The Brazil Data Cube (BDC) is being developed by Brazil’s National Institute for Space Research (INPE). Its goal is to create multidimensional data cubes of analysis-ready data Brazil. The BDC uses three hierarchical grids based on the Albers Equal Area projection and SIRGAS 2000 datum. The three grids are generated taking -54∘longitude as the central reference and defining tiles of 6 × 4, 3 × 2 and 1.5 × 1 degrees. The large grid is composed by tiles of 672 × 440 km2 and is used for CBERS-4 AWFI collections at 64 meter resolution; each CBERS-4 AWFI tile contains images of 10, 504 × 6, 865 pixels. The medium grid is used for Landsat-8 OLI collections at 30 meter resolution; tiles have an extension of 336 × 220   km2 and each image has 11,204 × 7,324 pixels. The small grid covers 168 × 110 km2 and is used for Sentinel-2 MSI collections at 10m resolutions; each image has 16,806 × 10,986 pixels. The data cubes in the BDC are regularly spaced in time and cloud-corrected.

```{r, out.width="50%", fig.cap="BDC grid. Hierarchical BDC tiling system showing overlayed on Brazilian Biomes (a), illustrating that one large tile (b) contains four medium tiles (c) and that medium tile contains four small tiles(source: authors of SITS)"}
knitr::include_graphics(here::here("_posts/2021-11-21-sits/bdc_grid.png"))
```

To access the Brazil Data Cube, users need to provide their credentials using environmental variables.

```{r}
Sys.setenv(
    "BDC_ACCESS_KEY" = "aeEELIvt9fq5QhaVoz0Yk9oir2gez2x7DwDVlmIRzX"
)
```

Creating a data cube using the BDC is similar to what is required for AWS. The user defines an image collection, a spatiotemporal extent, bands, and optionally a bounding box. In the example below, the data cube is defined as one tile (“022024”) of “CB4_64_16D_STK-1” collection which holds CBERS AWFI images at 16 days resolution. Other collections include “LC8_30_16D_STK-1” (Landsat OLI images at 16 days), “S2-SEN2COR_10_16D_STK-1” (Sentinel-2 MSI images at 16 days with 10 meter resolution) and “MOD13Q1-6” (MODIS MOD13SQ1 product, collection 6).

```{r}
cbers_tile <- sits_cube(
    source = "BDC",
    collection = "CB4_64_16D_STK-1",
    name = "cbers_022024",
    bands = c("NDVI", "EVI"),
    tiles = "022024",
    start_date = "2018-09-01",
    end_date = "2019-08-28"
)
```


### 1.3.3 Defining a data cube using files

In some cases, users have downloaded files from image collections and have them available in their computer or in a local network. As sits does not have access to STAC information that describe the files, they should be organized and named to allow SITS to create a data cube.

All files should be in the same directory and have the same spatial resolution and projection. Each file should contain a single image band for a single date. Since raster files in popular formats (e.g., GeoTiff and JPEG 2000) do not include temporal and band information, each file name needs to include date and band. Information on the tile reference system should be provided. Also, When building data cubes from images stored in a local machine, users need to provide information about the original source from with the data was downloaded. The reason to included such information is because there are no standards for the metadata used to process an image.

When working with local cubes, the following parameters should be provided to the sits_cube() function:

* source - value should be “LOCAL”;
* name - internal name for the data cube (free user choice);
* origin - name of the original data (possible values are “BDC” (Brazil Data Cube), “AWS” (Amazon Web Services), or “DEA” (Digital Earth Africa));
* collection - name of the collection from where the data was extracted. This should be “MOD13Q1-6,” “CB4_64_16D_STK-1,” “LC8_30_16D_STK-1” and “S2-SEN2COR_10_16D_STK-1,” respectively, for CBERS-4, LANDSAT-8, and SENTINEL-2 in the case of the BDC. For “AWS,” the only collection currently supported is “SENTINEL-S2-L2A.” In the case of “DEA,” the supported collection is “S2_L2A.” More collections will be added in future versions;
* data_dir - directory where the image data is located;
* parse_info - information to parse the file names and extract the information on the tile, date and band associated with each individual file. It is assumed that file names contain image descriptors separated by a delimiter (usually "_"). For example, CBERS-4_AWFI_022024_B13_2018-02-02.tif and SENTINEL-2_MSI_L20KP_20m_B08_2021_03_29.jp2 are valid file names for sits. The parse_info parameter is a list of strings indicating at which position the names of tile, date and band are to be found. In the two file names above, the parsing info is respectively c("X1", "X2", "tile", band", "date") and c("X1", "X2", "tile", "X3", "band", "date");
* delim - separator character between descriptions in the file name (default is "_").

The following example shows how to define a data cube based on local files available as part of the sits package.


```{r}
library(sits)
# Create a cube based on a stack of CBERS data
data_dir <- system.file("extdata/CBERS", package = "sitsdata")

# files are named using the convention 
# "CB4_64_16D_STK_022024_2018-08-29_2018-09-13_EVI.tif"
cbers_cube <- sits_cube(
      source = "LOCAL",
      name = "022024",
      origin = "BDC",
      collection = "MOD13Q1-6",
      data_dir = data_dir,
      delim = "_",
      parse_info = c("X1", "X2", "X3", "X4", "tile", "date", "X5", "band")
)
```

```{r}
# show the timeline of the cube
sits_timeline(cbers_cube)
```

```{r}
# show the bands of the cube
sits_bands(cbers_cube)
```

```{r}
# plot the band B16 (EVI) in the first time instance
plot(cbers_cube, band = "EVI", time = 1)
```
```{r}
# plot the band B16 (NDVI) in the first time instance
plot(cbers_cube, band = "NDVI", time = 1)
```

## 1.4 Regularizing data cubes

Analysis-ready data (ARD) collections available in AWS and DE Africa do not have consistent timelines. In general, images in neighboring tiles have different timelines. This is a problem when classifying large areas. In this case, users may want to produce data cubes with regular time intervals. For example, a user may want to define the best Sentinel-2 pixel in a one-month period. This can be done in sits by the function sits_regularize(), which uses the package gdalcubes [3]. For details in gdalcubes, please see https://github.com/appelmar/gdalcubes.

```{r}
# creating a data cube in AWS
s2_cube <- sits_cube(source = "AWS",
                     name = "T20LKP_2018_2019",
                     collection = "sentinel-s2-l2a",
                     tiles = c("20LKP", "20LLP"),
                     start_date = as.Date("2018-07-01"),
                     end_date = as.Date("2018-08-31"),
                     bands = c("B11", "CLOUD"),
                     s2_resolution = 60
)
```

```{r}
# list the timeline of the AWS S2 cube (there are 12 images)
sits_timeline(s2_cube)
```



```{r}
# regularize the cube to one month intervals
reg_cube <- sits_regularize(
          cube       = s2_cube,
          #name       = "T20LKP_20LKP_2OLLP_15D",
          output_dir = tempdir(),
          res = 60,
          period     = "P15D",
          agg_method = "median",
          cloud_mask = TRUE,
          multicores = 4
)
```



In the above example, the user has selected the s2_cube object defined using AWS (see example above). As described earlier in this chapter, because of the way ARD image collections are built, the timelines of tiles “20LLP” and “20LKP” associated with this cube are different. The sits_regularize() function builds a new data cube, with the same temporal extent as the s2_cube but with the same timeline. In this function, the period parameter controls the temporal interval between two images. Values should abide by the ISO8601 time period specification, which states that time interval should be defined as “P[n]Y[n]M[n]D,” where Y stands for “years,” “M” for months and “D” for days. Thus, “P1M” stands for a one-month period, “P15D” for a fifteen-day period.

When joining different images to get the best image for a period, sits_regularize() uses an aggregation method, defined by the parameter agg_method. It specifies how individual values of different pixels should be combined. The default is median, which select the most frequent value for the pixel during the desired interval. For more details, see ?sits_regularize.

# 2.1 Data structures for satellite time series

```{r}
# data set of samples
library(sits)
data("samples_matogrosso_mod13q1")
samples_matogrosso_mod13q1[1:3,]
```
A sits tibble contains data and metadata. The first six columns contain the metadata: spatial and temporal information, the label assigned to the sample, and the data cube from where the data has been extracted. The spatial location is given in longitude and latitude coordinates for the “WGS84” ellipsoid. For example, the first sample has been labeled "Cerrado, at location (-58.5631, -13.8844) and is considered valid for the period (2007-09-14, 2008-08-28). Informing the dates where the label is valid is crucial for correct classification. In this case, the researchers involved in labeling the samples chose to use the agricultural calendar in Brazil, where the spring crop is planted in the months of September and October, and the autumn crop is planted in the months of February and March. For other applications and other countries, the relevant dates will most likely be different from those used in the example. The time_series column contains the time series data for each spatiotemporal location. This data is also organized as a tibble, with a column with the dates and the other columns with the values for each spectral band.

# 2.2 Utilities for handling time series

The package provides functions for data manipulation and displaying information for sits tibble. For example, sits_labels_summary() shows the labels of the sample set and their frequencies.

```{r}
sits_labels_summary(samples_matogrosso_mod13q1)
```

In many cases, it is helpful to relabel the data set. For example, there may be situations when one wants to use a smaller set of labels, since samples in one label on the original set may not be distinguishable from samples with other labels. We then could use sits_relabel(), which requires a conversion list (for details, see ?sits_relabel).

Given that we have used the tibble data format for the metadata and the embedded time series, one can use the functions from dplyr, tidyr, and purrr packages of the tidyverse [4] to process the data. For example, the following code uses sits_select() to get a subset of the sample data set with two bands (NDVI and EVI) and then uses the dplyr::filter() to select the samples labelled either as “Cerrado” or “Pasture.”

```{r}
# select NDVI band
samples_ndvi <- sits_select(samples_matogrosso_mod13q1, 
                            bands = "NDVI")

# select only samples with Cerrado label
samples_cerrado <- dplyr::filter(samples_ndvi, label == "Cerrado")
```


# 2.3 Time series visualisation

Given a small number of samples to display, plot() tries to group as many spatial locations together. In the following example, the first 12 samples of “Cerrado” class refer to the same spatial location in consecutive time periods. For this reason, these samples are plotted together.

```{r}
# plot the first 12 samples
plot(samples_cerrado[1:12,])
```
For a large number of samples, where the number of individual plots would be substantial, the default visualization combines all samples together in a single temporal interval (even if they belong to different years). All samples with the same band and label are aligned to a common time interval. This plot is useful to show the spread of values for the time series of each band. The strong red line in the plot shows the median of the values, while the two orange lines are the first and third interquartile ranges. The documentation of plot.sits() has more details about the different ways it can display data.

```{r}
# plot all cerrado samples together
plot(samples_cerrado)
```


# 2.4 Obtaining time series data from data cubes

To get a time series in sits, one has to create a data cube, as described previously. Users can request one or more time series points from a data cube by using sits_get_data(). This function provides a general means of access to image time series. Given a data cube, the user provides the latitude and longitude of the desired location, the bands, and the start date and end date of the time series. If the start and end dates are not provided, it retrieves all the available periods. The result is a tibble that can be visualized using plot().

```{r}
# Obtain a raster cube with 23 instances for one year
# Select the band "ndvi", "evi" from images available in the "sitsdata" package
data_dir <- system.file("extdata/sinop", package = "sitsdata")
```


```{r}
# create a raster metadata file based on the information about the files
raster_cube <- sits_cube(
    source     = "LOCAL",
    origin     = "BDC",
    collection = "MOD13Q1-6",
    name       = "Sinop",
    data_dir   = data_dir,
    parse_info = c("X1", "X2", "tile", "band", "date")
)
```

```{r}
# a point in the transition forest to pasture in Northern MT
# obtain a time series from the raster cube for this point
series <- sits_get_data(cube      = raster_cube,
                        longitude = -55.57320, 
                        latitude  = -11.50566,
                        bands     = c("NDVI", "EVI"))
```

```{r}
plot(series)
```

A useful case is when a set of labelled samples are available to be used as a training data set. In this case, one usually has trusted observations that are labelled and commonly stored in plain text files in comma-separated values (CSV) or using shapefiles (SHP). Function sits_get_data() takes a CSV or SHP file path as an argument. For each training sample, CSV files should provide latitude and longitude, start and end dates that define the temporal bounds, and a label associated with a ground sample. An example of a CSV file used is shown below.

```{r}
# retrieve a list of samples described by a CSV file
samples_csv_file <- system.file("extdata/samples/samples_sinop_crop.csv",
                           package = "sits")
# for demonstration, read the CSV file into an R object
samples_csv <- read.csv(samples_csv_file)
# print the first three lines
samples_csv[1:3,]
```

The main difference between the files used by sits to retrieve training samples from those used traditionally in remote sensing data analysis is that users are expected to provide the temporal information (start_date and end_date). In the simplest case, all samples share the same dates. That is not a strict requirement. Users can specify different dates, as long as they have a compatible duration. For example, the data set samples_modis_4bands provided with the sits package contains samples from different years covering the same duration. These samples were obtained from the MOD13Q1 product, which contains the same number of images per year. Thus, all time series in the data set samples_modis_4bands have the same number of instances.

```{r}
samples_modis_4bands[1:5,]
```
Given a suitably built CSV sample file, sits_get_data() requires two parameters: (a) cube, the name of the R object that describes the data cube; (b) file, the name of the CSV file.

```{r}
# get the points from a data cube in raster brick format
points <- sits_get_data(cube = raster_cube, file = samples_csv_file)
# show the tibble with the first three points
points[1:3,]
```

Users can also specify samples by providing shapefiles in point or polygon format. In this case, the geographical location is inferred from the geometries associated with the shapefile. For files containing points, the geographical location is obtained directly; for files with polygon, the parameter .n_shp_pol (defaults to 20) determines the number of samples to be extracted from each polygon. The temporal information is inferred from the data cube from which the samples are extracted or can be provided explicitly by the user. The label information is taken from the attribute file associated with the shapefile. The parameter shp_attr indicates the name of the column which contains the label to be associated with each time series.

```{r}
# define the input shapefile
shp_file <- system.file("extdata/shapefiles/agriculture/parcel_agriculture.shp", 
                        package = "sits")

# set the start and end dates 
start_date <- "2013-09-14"
end_date   <- "2014-08-29"

# define the attribute name that contains the label
shp_attr <- "ext_na"

# define the number of samples to extract from each polygon
.n_shp_pol <- 10
```

```{r}
# read the points in the shapefile and produce a CSV file
samples <- sits_get_data(cube       = raster_cube, 
                         file       = shp_file, 
                         start_date = start_date, 
                         end_date   = end_date, 
                         shp_attr   = shp_attr, 
                         .n_shp_pol = .n_shp_pol)
samples[1:3,]
```

# 2.5 Filtering techniques for time series

Satellite image time series generally is contaminated by atmospheric influence, geolocation error, and directional effects [5]. Atmospheric noise, sun angle, interferences on observations or different equipment specifications, as well as the very nature of the climate-land dynamics can be sources of variability [6]. Inter-annual climate variability also changes the phenological cycles of the vegetation, resulting in time series whose periods and intensities do not match on a year-to-year basis. To make the best use of available satellite data archives, methods for satellite image time series analysis need to deal with noisy and non-homogeneous data sets. In this vignette, we discuss filtering techniques to improve time series data that present missing values or noise.

The literature on satellite image time series has several applications of filtering to correct or smooth vegetation index data. The package supports the well-known Savitzky–Golay (sits_sgolay()) and Whittaker (sits_whittaker()) filters. In an evaluation of MERIS NDVI time series filtering for estimating phenological parameters in India, [6] found that the Whittaker filter provides good results. [7] found that the Savitzky-Golay filter is good for reconstruction in tropical evergreen broadleaf forests.

## 2.5.1 Savitzky–Golay filter

```{r}
# Take NDVI band of the first sample data set
point_ndvi <- sits_select(point_mt_6bands, band = "NDVI")
# apply Savitzky–Golay filter
point_sg <- sits_sgolay(point_ndvi, length = 15)

# merge the point and plot the series
sits_merge_plot <- sits_merge(point_sg, point_ndvi) %>% plot()
```

Notice that the resulting smoothed curve has both desirable and unwanted properties. For the period 2000 to 2008, the Savitsky-Golay filter remove noise resulting from clouds. However, after 2010, when the region has been converted to agriculture, the filter removes an important part of the natural variability from the crop cycle. Therefore, the length parameter is arguably too big and results in oversmoothing. Users can try to reduce this parameter and analyse the results.

## 2.5.2 Whittaker filter

The following example shows the effect of Whitakker filter on a point extracted from the MOD13Q1 product, ranging from 2000-02-18 to 2018-01-01. The lambda parameter controls the smoothing of the filter. By default, it is set to 0.5, a small value. For illustrative purposes, we show the effect of a larger smoothing parameter

```{r}
# Take NDVI band of the first sample data set
point_ndvi <- sits_select(point_mt_6bands, band = "NDVI")
# apply Whitakker filter
point_whit <- sits_whittaker(point_ndvi, lambda = 8)
# merge the point and plot the series
sits_merge(point_whit, point_ndvi) %>% plot()
```
In the same way as what is observed in the Savitsky-Golay filter, high values of the smoothing parameter lambda produce an oversmoothed time series that reduces the capacity of the time series to represent natural variations on crop growth. For this reason, low smoothing values are recommended when using the sits_whittaker function.



# Time Series Clustering to Improve the Quality of Training Samples

One of the key challenges when using samples to train machine learning classification models is assessing their quality. Noisy and imperfect training samples can have a negative effect on classification performance. Therefore, it is useful to apply pre-processing methods to improve the quality of the samples and to remove those that might have been wrongly labeled or that have low discriminatory power. sits provides two clustering methods to improve sample quality: agglomerative hierarchical clustering (AHC) and self-organizing maps (SOM).


# 3.1 Clustering for sample quality control: overview

Experience with machine learning methods has established that the limiting factor in obtaining good results is the number and quality of training samples. Large and accurate data sets are better, no matter the algorithm used [12]; noisy training samples can have a negative effect on classification performance [13].

When assessing the quality of training samples, it is useful to distinguish between samples that have been wrongly labelled and differences that result from natural variability of class signatures. When training data is collected over a large geographic region, natural variability of vegetation phenology can result in different patterns being assigned to the same label. Phenological patterns can vary spatially across a region and are strongly correlated with climate variations. A related issue is the limitation of crisp boundaries to describe the natural world. Class definition use idealized descriptions (e.g., “a savanna woodland has tree cover of 50% to 90% ranging from 8 to 15 meters in height”). In practice, the boundaries between classes are fuzzy and sometimes overlap, making it hard to distinguish between them. Therefore, it is useful to apply pre-processing methods to improve the quality of the samples and to remove those that might have been wrongly labeled or that have low discriminatory power. Representative samples lead to good classification maps. The package provides support for two clustering methods to test sample quality: (a) Agglomerative Hierarchical Clustering (AHC); (b) Self-organizing Maps (SOM).

The two methods have different computational complexities. As discussed below, AHC results are somewhat easier to interpret than those of SOM. However, AHC has a computational complexity of  
O
(
n
2
)
  given the number of time series  
n
 , whereas SOM complexity is linear with respect to n. Therefore, for large data sets, AHC requires an substantial amount of memory and running time; in these cases, SOM is recommended.
 
 
 
# 3.2 Hierachical clustering for sample quality control

Agglomerative hierarchical clustering (AHC) computes the dissimilarity between any two elements from a data set. Depending on the distance functions and linkage criteria, the algorithm decides which two clusters are merged at each iteration. This approach is useful for exploring data samples due to its visualization power and ease of use [14]. In sits, AHC is implemented using sits_cluster_dendro().

```{r}
# take a set of patterns for 2 classes
# create a dendrogram, plot, and get the optimal cluster based on ARI index
clusters <- sits_cluster_dendro(samples = cerrado_2classes, 
                                bands = c("NDVI", "EVI"),
                                dist_method = "dtw_basic",
                                linkage =  "ward.D2"
)
```



The sits_cluster_dendro() function has one mandatory parameter (samples), where users should provide the name of the R object containing the data samples to be evaluated. Optional parameters include bands, dist_method and linkage. The dist_method parameter specifies how to calculate the distance between two time series. We recommend a metric that uses dynamic time warping (DTW)[15], as DTW is reliable method for measuring differences between satellite image time series [16]. The options available in sits are based on those provided by package dtwclust, which include “dtw_basic,” “dtw_lb,” and “dtw2.” Please check ?dtwclust::tsclust for more information on DTW distances.

The linkage parameter defines the metric used for computing the distance between clusters. The recommended linkage criteria are: “complete” or “ward.D2.” Complete-linkage prioritizes the within-cluster dissimilarities, producing clusters with shorter distance samples. Complete-linkage clustering can be sensitive to outliers, which can increase the resulting intracluster data variance. As an alternative, Ward proposes criteria to minimize the data variance by means of either sum-of-squares or sum-of-squares-error [17]. Ward’s intuition is that clusters of multivariate observations, such as time series, should be approximately elliptical in shape [18].

After creating a dendrogram, an important question emerges: where to cut the dendrogram? The answer depends on what are the purposes of the cluster analysis, which needs to balance two objectives: get clusters as large as possible, and get clusters as homogeneous as possible with respect to their known classes. The sits_cluster_dendro() function computes the adjusted rand index (ARI) for a series of the different number of generated clusters. This function returns the height where the cut of the dendrogram maximizes the index. For more detaily, please see [19].

In the example above after calculating the dendrogram, the ARI index indicates that six (6) clusters are the best possible arrangement. However, these clusters may still contain a mixed composition of samples of different classes.

The result of the sits_cluster operation is a sits_tibble with one additional column, called “cluster.” The function sits_cluster_frequency() provides information on the composition of each cluster,


```{r}
clusters
```


```{r}
# show clusters samples frequency
sits_cluster_frequency(clusters)
```


The result shows that the clusters have a predominance of either “Cerrado” or “Pasture” classes with the exception of cluster  
3
 . The contingency table plotted by sits_cluster_frequency() shows how the samples are distributed across the clusters and helps identify two kinds of problems. The first is relative to small amounts of samples in clusters dominated by another class (e.g. clusters  
1
 ,  
2
 ,  
4
 ,  
5
 , and  
6
 ), while the second is relative to those samples in non-dominated clusters (e.g. cluster  
3
 ). These confusions can be an indication of samples with poor quality, and inadequacy of selected parameters for cluster analysis, or even a natural confusion due to the inherent variability of the land classes.

It is possible to remove clusters with mixed classes using the dplyr package. In the example above, removing cluster  
3
  can be done using the dplyr::filter() function.
  
```{r}
# remove cluster 3 from the samples
clusters_new <- dplyr::filter(clusters, cluster != 3)

# show new clusters samples frequency
sits::sits_cluster_frequency(clusters_new)
```

The resulting clusters still contained mixed labels, possibly resulting from outliers. In this case, users may want to remove the outliers and leave only the most frequent class. To do this, one can use sits_cluster_clean(), which removes all minority samples, as shown below.

```{r}
# clear clusters, leaving only the majority class in each cluster
clean <- sits_cluster_clean(clusters_new)
# show clusters samples frequency
sits_cluster_frequency(clean)
```

After cleaning the samples using dendrograms, users are expected to have a better set of samples which will provide more accurate estimates of land classification.


# 3.3 Using self-organizing maps for sample quality control

As an alternative for hierarchical clustering for quality control of training samples, SITS provides a clustering technique based on self-organizing maps (SOM). SOM is a dimensionality reduction technique [20], where high-dimensional data is mapped into a two dimensional map, keeping the topological relations between data patterns. As the shown in the Figure below, the SOM 2D Map is composed by units called . Each neuron has a weight vector, with the same dimension as the training samples. At the start, neurons are assigned a small random value and then trained by competitive learning. The algorithm computes the distances of each member of the training set to all neurons and finds the neuron closest to the input, called the best matching unit (BMU).

```{r, out.width="50%", fig.cap="SOM 2D map creation (source: Santos et al.(2021)"}
knitr::include_graphics(here::here("_posts/2021-11-21-sits/som_structure.png"))
```































